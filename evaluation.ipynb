{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f49f42ff700>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformer_lens import HookedTransformer\n",
    "from interpolated_ffn import ModelWithBilinearLayer, load_layer\n",
    "from sae_lens import SAE\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import IFrame\n",
    "import safetensors\n",
    "import einops\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72930e0ab76f45ee8aae07ebf61ca770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "model_name = \"gemma-2-2b\"\n",
    "model_pretrained = HookedTransformer.from_pretrained_no_processing(model_name, device = device, dtype=dtype)\n",
    "layer = 18\n",
    "\n",
    "# Bilinear layer with original weights \n",
    "model_bilinear = ModelWithBilinearLayer(model_pretrained, layer)\n",
    "\n",
    "# Bilinear layers with different finetuning strategies\n",
    "model_bilinear_logit_mse = ModelWithBilinearLayer(model_pretrained, layer)\n",
    "model_bilinear_logit_mse.ffn.load_layer(\"layer-18-step-20000-logit-mse.safetensors\")\n",
    "model_bilinear_output_mse = ModelWithBilinearLayer(model_pretrained, layer)\n",
    "model_bilinear_output_mse.ffn.load_layer(\"layer-18-step-20000-output-mse.safetensors\")\n",
    "model_bilinear_ce = ModelWithBilinearLayer(model_pretrained, layer)\n",
    "model_bilinear_ce.ffn.load_layer(\"layer-18-step-20000-ce.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.memmap('data_train.bin', dtype=np.uint32, mode='r')\n",
    "\n",
    "def sample_batch(size, seq_len):\n",
    "    indices = torch.randint(len(data) - seq_len - 1, (size,))\n",
    "    xs = torch.stack([torch.from_numpy(data[i:i+seq_len].astype(np.int64)) for i in indices])\n",
    "    ys = torch.stack([torch.from_numpy(data[i+1:i+seq_len+1].astype(np.int64)) for i in indices])\n",
    "    return xs.to(device), ys.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the performance of the different versions of the model on samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained: 3.6328125\n",
      "Bilinear untuned: 3.80359375\n",
      "Bilinear logit reconstruction: 3.66703125\n",
      "Bilinear layer output reconstruction: 3.71015625\n",
      "Bilinear cross-entropy on next token prediction: 3.1803125\n"
     ]
    }
   ],
   "source": [
    "def eval_model(model):\n",
    "    torch.manual_seed(12345)\n",
    "    seq_len = 1024\n",
    "    batch_size = 25\n",
    "    batches = 100\n",
    "    loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i in range(batches):\n",
    "            tokens, next_tokens = sample_batch(batch_size, seq_len)\n",
    "            logits = model(tokens)\n",
    "            loss += F.cross_entropy(logits.view(-1, logits.size(-1)), next_tokens.view(-1)).item()\n",
    "    return loss / batches\n",
    "\n",
    "print(\"Pretrained:\", eval_model(model_pretrained))\n",
    "print(\"Bilinear untuned:\", eval_model(model_bilinear))\n",
    "print(\"Bilinear logit reconstruction:\", eval_model(model_bilinear_logit_mse))\n",
    "print(\"Bilinear layer output reconstruction:\", eval_model(model_bilinear_output_mse))\n",
    "print(\"Bilinear cross-entropy on next token prediction:\", eval_model(model_bilinear_ce))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAE(\n",
       "  (activation_fn): ReLU()\n",
       "  (hook_sae_input): HookPoint()\n",
       "  (hook_sae_acts_pre): HookPoint()\n",
       "  (hook_sae_acts_post): HookPoint()\n",
       "  (hook_sae_output): HookPoint()\n",
       "  (hook_sae_recons): HookPoint()\n",
       "  (hook_sae_error): HookPoint()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res-canonical\",\n",
    "    sae_id = f\"layer_{layer}/width_16k/canonical\",\n",
    "    device = device\n",
    ")\n",
    "sae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the performance of the SAE with the different versions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained: (5.0151436281204225, 67.59741718750003)\n",
      "Bilinear untuned: (7.057553396224976, 42.397410937500005)\n",
      "Bilinear logit reconstruction: (6.138280916213989, 53.94823281249998)\n",
      "Bilinear output reconstruction: (5.741299681663513, 56.174267578124976)\n",
      "Bilinear cross-entropy on next token prediction: (6.763096032142639, 57.730014843749984)\n"
     ]
    }
   ],
   "source": [
    "def reconstruction_loss_and_l0(run_fn):\n",
    "    batches = 100\n",
    "    batch_size = 25\n",
    "    seq_len = 1024\n",
    "    loss = 0.0\n",
    "    l0 = 0.0\n",
    "    for i in range(batches):\n",
    "        batch, _ = sample_batch(batch_size, seq_len)\n",
    "        res = run_fn(batch)\n",
    "\n",
    "        res = res.view(-1, res.size(-1))\n",
    "        batch = batch.view(-1)\n",
    "\n",
    "        res = res[batch != model_pretrained.tokenizer.bos_token_id]\n",
    "\n",
    "        feature_activations = sae.encode(res)\n",
    "        reconstructed = sae.decode(feature_activations)\n",
    "\n",
    "        l0 += (feature_activations > 0.0).sum().item() / (batch_size * seq_len)\n",
    "        activations = None\n",
    "        loss += F.mse_loss(res, reconstructed).item()\n",
    "\n",
    "    return loss / batches, l0 / batches\n",
    "\n",
    "def get_activations_bilinear(model, tokens):\n",
    "    x = model.model(tokens, stop_at_layer=layer)\n",
    "    return model.newlayer(x)\n",
    "\n",
    "print(\"Pretrained:\", reconstruction_loss_and_l0(lambda x: model_pretrained(x, stop_at_layer=layer+1)))\n",
    "print(\"Bilinear untuned:\", reconstruction_loss_and_l0(lambda x: get_activations_bilinear(model_bilinear, x)))\n",
    "print(\"Bilinear logit reconstruction:\", reconstruction_loss_and_l0(lambda x: get_activations_bilinear(model_bilinear_logit_mse, x)))\n",
    "print(\"Bilinear output reconstruction:\", reconstruction_loss_and_l0(lambda x: get_activations_bilinear(model_bilinear_output_mse, x)))\n",
    "print(\"Bilinear cross-entropy on next token prediction:\", reconstruction_loss_and_l0(lambda x: get_activations_bilinear(model_bilinear_ce, x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below calculates the interaction matrix, as described in [this paper](https://arxiv.org/pdf/2406.03947)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9216])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0447e-03, -2.8381e-03,  1.1749e-03,  ...,  2.5024e-03,\n",
       "          9.3842e-04, -5.4932e-04],\n",
       "        [ 2.6703e-05, -3.6621e-04,  3.2959e-03,  ..., -1.5411e-03,\n",
       "          1.2817e-03, -2.0695e-04],\n",
       "        [ 8.6212e-04, -2.3346e-03, -2.7008e-03,  ...,  5.9891e-04,\n",
       "          2.1210e-03,  9.6893e-04],\n",
       "        ...,\n",
       "        [ 7.2098e-04, -9.1553e-04, -2.7161e-03,  ...,  2.5024e-03,\n",
       "          1.7014e-03, -5.5313e-04],\n",
       "        [-4.6539e-04, -3.2959e-03, -4.8828e-04,  ...,  5.5313e-04,\n",
       "         -1.3733e-03, -1.1673e-03],\n",
       "        [ 1.3123e-03, -8.3542e-04,  3.3722e-03,  ..., -2.2278e-03,\n",
       "         -3.5095e-04, -5.5695e-04]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = model_bilinear_logit_mse.ffn\n",
    "\n",
    "def interaction_matrix(u):\n",
    "    v = ffn.V @ u\n",
    "    interaction_matrices = einops.einsum(ffn.W1[:, :4608], ffn.W2[:, :4608], v[:4608], 'i j, k j, j -> i k') + einops.einsum(ffn.W1[:, 4608:], ffn.W2[:, 4608:], v[4608:], 'i j, k j, j -> i k')\n",
    "    return interaction_matrices\n",
    "\n",
    "def symm_interaction_matrix(u):\n",
    "    m = interaction_matrix(u)\n",
    "    return m + m.T\n",
    "\n",
    "u = torch.randn(2304).to(device=device, dtype=dtype)\n",
    "symm_interaction_matrix(u)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aisf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
